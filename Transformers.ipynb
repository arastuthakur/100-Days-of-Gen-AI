{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers Architecture"
      ],
      "metadata": {
        "id": "-6j8-SU8lu14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "p4cosVoUlzw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LfB9FnKRl3FB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a simple Transformer model**"
      ],
      "metadata": {
        "id": "w_vRQsEll6aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "metadata": {
        "id": "pIOjRLj4l5ka"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Encoder**"
      ],
      "metadata": {
        "id": "7wQ6LUI4mBdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "id": "WIRqIykumAe-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Encoder Layer**"
      ],
      "metadata": {
        "id": "7cR8E568mIkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Encoder Layer\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "metadata": {
        "id": "KUj-0BH2mMzz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Decoder**"
      ],
      "metadata": {
        "id": "RBkP83NdmO56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "metadata": {
        "id": "z09gqxX5mR1j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the Decoder Layer**"
      ],
      "metadata": {
        "id": "oLYkArAXmYXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "7D-aXBErmXsA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define MultiHeadAttention**"
      ],
      "metadata": {
        "id": "KUtB3OVSmff4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "Ycbpo7HymOAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the point wise feed forward network**"
      ],
      "metadata": {
        "id": "MP30hFdZmpRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "metadata": {
        "id": "3xFrBYbDmsVy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define positional encoding**"
      ],
      "metadata": {
        "id": "_rNIFRbMmxgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "A3augqpVmukx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate sample data**"
      ],
      "metadata": {
        "id": "T3DryTWOm4MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
        "    input_vocab_size=8500, target_vocab_size=8000,\n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "# Sample input\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "# Sample target\n",
        "temp_target = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "# Sample training mask\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(temp_input, temp_target)\n",
        "\n",
        "# Forward pass\n",
        "predictions, attention_weights = sample_transformer(temp_input, temp_target, training=False,\n",
        "                                                    enc_padding_mask=enc_padding_mask,\n",
        "                                                    look_ahead_mask=look_ahead_mask,\n",
        "                                                    dec_padding_mask=dec_padding_mask)"
      ],
      "metadata": {
        "id": "jhwFfdCum1-M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting attention weights**"
      ],
      "metadata": {
        "id": "kYNKlMrgm_aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "    sentence = \" \".join([str(x) for x in sentence])\n",
        "    result = \" \".join([str(x) for x in result])\n",
        "\n",
        "    attention = attention[layer]\n",
        "\n",
        "    num_heads = attention.shape[1]\n",
        "    num_columns = min(num_heads, 8)\n",
        "    num_rows = (num_heads - 1) // num_columns + 1\n",
        "\n",
        "    for head in range(num_heads):\n",
        "        ax = fig.add_subplot(num_rows, num_columns, head + 1)\n",
        "\n",
        "        # plot the attention weights\n",
        "        ax.matshow(attention[0][head][:-1, :], cmap='viridis')\n",
        "\n",
        "        fontdict = {'fontsize': 10}\n",
        "\n",
        "        ax.set_xticks(range(len(sentence) + 2))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "\n",
        "        ax.set_ylim(len(result) - 1.5, -0.5)\n",
        "\n",
        "        ax.set_xticklabels(['<start>'] + [w for w in sentence] + ['<end>'],\n",
        "                           fontdict=fontdict, rotation=90)\n",
        "\n",
        "        ax.set_yticklabels([w for w in result if w != '<end>'],\n",
        "                           fontdict=fontdict)\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(head + 1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_attention_weights(attention_weights, temp_input[0], temp_target[0], 'decoder_layer2_block2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "UXsNajrfm7S8",
        "outputId": "8d5e4558-9c0c-43ce-fa7c-ad75671c6ee7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x800 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LSrgc8senCjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}